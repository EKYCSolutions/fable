# FABLE

**FABLE** is an automated face accessory labeling tool powered by a **vision-language model (VLM)**. It scans through images, detects faces (optional), and uses a multimodal LLM to describe whether certain accessories (like glasses, masks, hats, etc.) are present.

This tool is particularly useful for generating labeled datasets from large collections of face images for training face accessories recognition models.

## üöÄ Features

- üîç **Automatic image scanning** ‚Äî recursively gathers all images from a directory  
- üß© **Configurable labeling schema** ‚Äî define accessories and descriptions in a YAML file  
- üß† **LLM-based annotation** ‚Äî integrates with **LangChain** and **Ollama** for structured labeling  
- üë§ **Optional face detection** ‚Äî detects individual faces with `face_recognition`  
- üßµ **Parallel processing** ‚Äî speeds up labeling with multi-threaded execution  
- üìù **Resumable progress tracking** ‚Äî continue from where you left off using the built-in tracker  
- üíæ **CSV output** ‚Äî all annotations saved in `annotation.csv`

## üì¶ Installation

> This tool uses **Langchain** to interacts with models running through [Ollama](https://ollama.ai/). Therefore, it requires **Ollama** to be installed and running locally.

1. Clone the repository:

```bash
git clone https://github.com/EKYCSolutions/fable.git
cd fable
```

2. (Optional) Create and activate a virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows use `.venv\Scripts\activate`
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

## ‚öôÔ∏è Configuration

Create a YAML configuration file (e.g., `config.yaml`) like this:

```yaml
accessories:
  glasses: Eyewear consisting of lenses mounted in a frame resting on the nose and ears.
  hat: Any item worn on or covering the head.
  mask: Cloth or medical covering over the mouth and nose.

configurations:
  model: qwen2-vl:7b
  detect_faces: true
  image_extensions:
    - jpg
    - jpeg
    - png
```

### Configuration fields

| Key | Type | Description |
|-----|------|-------------|
| `accessories` | dict | Mapping of label names to their descriptions |
| `configurations.model` | str | Ollama model name (e.g. `gemma3:12b`, `qwen2-vl:7b`) |
| `configurations.detect_faces` | bool | Whether to run face detection before labeling |
| `configurations.image_extensions` | list[str] | File extensions to include when searching for images |

## üß© Usage

Run the tool from the command line:

```bash
python label.py /path/to/images -c config.yaml -o output_dir
```

### Arguments

| Argument | Description | Default |
|-----------|--------------|----------|
| `data_dir` | Path to image directory | ‚Äî |
| `-c`, `--config` | Path to YAML configuration file | `config.yaml` |
| `-o`, `--output_dir` | Directory to store results and tracker files | `out/` |
| `-w`, `--workers` | Number of concurrent workers (threads) | `4` |
| `-v`, `--verbose` | Enable verbose logging | `False` |

## üß† Example Workflow

```bash
# Step 1: Prepare your dataset
mkdir data
cp /path/to/faces/*.jpg data/

# Step 2: Write config.yaml
nano config.yaml  # or use the example above

# Step 3: Start labeling
python label.py data -c config.yaml -o results -w 8
```

Progress is displayed in a live progress bar using **tqdm**:

```
Processing items:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [03:12<01:14,  2.37s/it]
```

If you stop and rerun, it will **resume automatically** without reprocessing finished images.

## üßæ Output

Each processed face is saved to a CSV file (default: `out/annotation.csv`) with the following structure:

| filename | person_id | xmin | ymin | xmax | ymax | glasses | mask | hat | ... |
|-----------|------------|------|------|------|------|----------|------|------|-----|
| `images/img1.jpg` | `0` | `32` | `48` | `128` | `144` | `1` | `0` | `0` | ... |

## üîç How It Works

1. **Loads configuration** ‚Üí from YAML  
2. **Scans image directory** ‚Üí finds all images by extension  
3. **Detects faces (optional)** ‚Üí via `face_recognition`  
4. **Converts each face to base64** ‚Üí for LLM input  
5. **Prompts the model** ‚Üí using LangChain and your accessories list  
6. **Parses structured output** ‚Üí into a Pydantic model  
7. **Appends results** ‚Üí to a CSV file  

## ‚ö° Tips for Better Performance

- Use a **smaller model** (e.g., `qwen2-vl:2b`) for faster annotation.
- Increase `-w` (workers) for better throughput on multicore machines.
- Disable face detection (`detect_faces: false`) if all images are already cropped faces.

## üßë‚Äçüíª Example Output (Simplified)

```csv
filename,person_id,xmin,ymin,xmax,ymax,glasses,mask,hat
data/image1.jpg,0,12,40,120,150,1,0,0
data/image2.jpg,0,10,30,130,140,0,1,0
```

## üß± Project Structure

```
fable/
‚îÇ
‚îú‚îÄ‚îÄ label.py               # Main labeling script
‚îú‚îÄ‚îÄ utils.py               # Helper functions
‚îú‚îÄ‚îÄ tracker.py             # Progress tracking
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ configs/               # Configs  directory.
    ‚îî‚îÄ‚îÄ config.yaml
```

## ü™™ License

Distributed under the MIT License. See [LICENSE](./LICENSE) for more information.